{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis for Steel Casting Defect Prediction\n",
    "\n",
    "**Date:** 2025-01-21  \n",
    "**Author:** Steel Defect Demo Team  \n",
    "**Purpose:** Comprehensive analysis of features for defect prediction\n",
    "\n",
    "## Overview\n",
    "This notebook provides a comprehensive analysis of the steel casting dataset, focusing on:\n",
    "- Feature distributions comparing normal vs defect casts\n",
    "- Correlation analysis with visualization\n",
    "- Feature importance ranking using multiple methods\n",
    "- Univariate and bivariate feature analysis\n",
    "- Feature selection recommendations\n",
    "- Publication-quality visualizations\n",
    "\n",
    "## Objectives\n",
    "- Understand feature distributions and their relationship to defect prediction\n",
    "- Identify the most important features for model training\n",
    "- Detect multicollinearity and provide feature selection guidance\n",
    "- Generate actionable insights for stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Analysis for Steel Casting Defect Prediction\n",
    "# Date: 2025-01-21\n",
    "# Author: Steel Defect Demo Team\n",
    "# Purpose: Comprehensive analysis of features for defect prediction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, pearsonr, spearmanr\n",
    "from sklearn.feature_selection import (\n",
    "    mutual_info_classif, SelectKBest, f_classif,\n",
    "    chi2, RFE\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data.data_loader import DataLoader\n",
    "from features.feature_engineer import CastingFeatureEngineer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(\"Feature Analysis Notebook - Steel Casting Defect Prediction\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned data\n",
    "data_loader = DataLoader(data_dir='../data')\n",
    "df = data_loader.load_cleaned_data('../data/processed/cleaned_data.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target distribution:\\n{df['defect'].value_counts()}\")\n",
    "print(f\"Target proportions:\\n{df['defect'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "missing_data = df.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    print(f\"\\nMissing values:\\n{missing_data[missing_data > 0]}\")\n",
    "else:\n",
    "    print(\"\\nNo missing values found\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('defect', axis=1)\n",
    "y = df['defect']\n",
    "\n",
    "# Feature types analysis\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE DISTRIBUTION ANALYSIS ===\n",
    "print(\"1. FEATURE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def analyze_feature_distributions(df, target_col='defect', max_features=20):\n",
    "    \"\"\"\n",
    "    Analyze feature distributions comparing normal vs defect casts\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Separate normal and defect samples\n",
    "    normal_data = df[df[target_col] == 0]\n",
    "    defect_data = df[df[target_col] == 1]\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in numeric_cols:\n",
    "        numeric_cols.remove(target_col)\n",
    "    \n",
    "    # Limit number of features for display\n",
    "    if len(numeric_cols) > max_features:\n",
    "        print(f\"Analyzing top {max_features} features (out of {len(numeric_cols)})\")\n",
    "        numeric_cols = numeric_cols[:max_features]\n",
    "    \n",
    "    for feature in numeric_cols:\n",
    "        # Statistical tests\n",
    "        stat_test = stats.mannwhitneyu(\n",
    "            normal_data[feature].dropna(),\n",
    "            defect_data[feature].dropna(),\n",
    "            alternative='two-sided'\n",
    "        )\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        normal_mean = normal_data[feature].mean()\n",
    "        defect_mean = defect_data[feature].mean()\n",
    "        pooled_std = np.sqrt(\n",
    "            ((len(normal_data[feature]) - 1) * normal_data[feature].var() +\n",
    "             (len(defect_data[feature]) - 1) * defect_data[feature].var()) /\n",
    "            (len(normal_data[feature]) + len(defect_data[feature]) - 2)\n",
    "        )\n",
    "        cohens_d = (defect_mean - normal_mean) / pooled_std if pooled_std > 0 else 0\n",
    "        \n",
    "        results[feature] = {\n",
    "            'normal_mean': normal_mean,\n",
    "            'defect_mean': defect_mean,\n",
    "            'normal_std': normal_data[feature].std(),\n",
    "            'defect_std': defect_data[feature].std(),\n",
    "            'mannwhitney_statistic': stat_test.statistic,\n",
    "            'mannwhitney_pvalue': stat_test.pvalue,\n",
    "            'cohens_d': cohens_d,\n",
    "            'effect_size': 'small' if abs(cohens_d) < 0.5 else 'medium' if abs(cohens_d) < 0.8 else 'large'\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "# Perform distribution analysis\n",
    "distribution_results = analyze_feature_distributions(df)\n",
    "\n",
    "# Sort by effect size\n",
    "distribution_results['abs_cohens_d'] = abs(distribution_results['cohens_d'])\n",
    "distribution_results = distribution_results.sort_values('abs_cohens_d', ascending=False)\n",
    "\n",
    "print(\"Top 10 features with largest effect sizes:\")\n",
    "display(distribution_results.head(10))\n",
    "\n",
    "# Visualize top discriminative features\n",
    "top_features = distribution_results.head(8).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(top_features):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot distributions\n",
    "    normal_data = df[df['defect'] == 0][feature]\n",
    "    defect_data = df[df['defect'] == 1][feature]\n",
    "    \n",
    "    ax.hist(normal_data, alpha=0.7, label='Normal', bins=30, density=True)\n",
    "    ax.hist(defect_data, alpha=0.7, label='Defect', bins=30, density=True)\n",
    "    \n",
    "    ax.set_title(f'{feature}\\nCohen\\'s d = {distribution_results.loc[feature, \"cohens_d\"]:.3f}')\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Distributions: Normal vs Defect Casts', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CORRELATION ANALYSIS ===\n",
    "print(\"2. CORRELATION ANALYSIS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "def create_correlation_analysis(df, target_col='defect'):\n",
    "    \"\"\"\n",
    "    Comprehensive correlation analysis\n",
    "    \"\"\"\n",
    "    # Prepare numeric data\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calculate correlation matrices\n",
    "    pearson_corr = numeric_df.corr(method='pearson')\n",
    "    spearman_corr = numeric_df.corr(method='spearman')\n",
    "    \n",
    "    # Target correlations\n",
    "    target_correlations = pd.DataFrame({\n",
    "        'pearson': pearson_corr[target_col].drop(target_col),\n",
    "        'spearman': spearman_corr[target_col].drop(target_col)\n",
    "    })\n",
    "    target_correlations['abs_pearson'] = abs(target_correlations['pearson'])\n",
    "    target_correlations = target_correlations.sort_values('abs_pearson', ascending=False)\n",
    "    \n",
    "    return pearson_corr, spearman_corr, target_correlations\n",
    "\n",
    "pearson_corr, spearman_corr, target_correlations = create_correlation_analysis(df)\n",
    "\n",
    "print(\"Top 15 features correlated with defect:\")\n",
    "display(target_correlations.head(15))\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Select top correlated features for visualization\n",
    "top_corr_features = target_correlations.head(20).index.tolist() + ['defect']\n",
    "corr_subset = pearson_corr.loc[top_corr_features, top_corr_features]\n",
    "\n",
    "# Create heatmap\n",
    "mask = np.triu(np.ones_like(corr_subset, dtype=bool))\n",
    "sns.heatmap(\n",
    "    corr_subset,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    fmt='.2f',\n",
    "    cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "\n",
    "plt.title('Feature Correlation Matrix (Top 20 + Target)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interactive correlation heatmap with Plotly\n",
    "fig = px.imshow(\n",
    "    corr_subset,\n",
    "    text_auto=True,\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    color_continuous_midpoint=0,\n",
    "    title=\"Interactive Correlation Heatmap\"\n",
    ")\n",
    "fig.update_layout(height=600, width=800)\n",
    "fig.show()\n",
    "\n",
    "# High correlation pairs (potential multicollinearity)\n",
    "def find_high_correlations(corr_matrix, threshold=0.8):\n",
    "    \"\"\"Find pairs of features with high correlation\"\"\"\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                high_corr_pairs.append({\n",
    "                    'feature1': corr_matrix.columns[i],\n",
    "                    'feature2': corr_matrix.columns[j],\n",
    "                    'correlation': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(high_corr_pairs)\n",
    "\n",
    "high_corr_pairs = find_high_correlations(pearson_corr, threshold=0.8)\n",
    "if not high_corr_pairs.empty:\n",
    "    print(f\"\\nHigh correlation pairs (|r| > 0.8): {len(high_corr_pairs)}\")\n",
    "    display(high_corr_pairs.head(10))\n",
    "else:\n",
    "    print(\"\\nNo high correlation pairs found (|r| > 0.8)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE IMPORTANCE ANALYSIS ===\n",
    "print(\"3. FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "def calculate_feature_importance(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Calculate feature importance using multiple methods\n",
    "    \"\"\"\n",
    "    importance_results = pd.DataFrame(index=X.columns)\n",
    "    \n",
    "    # 1. Random Forest Importance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    rf.fit(X, y)\n",
    "    importance_results['random_forest'] = rf.feature_importances_\n",
    "    \n",
    "    # 2. Mutual Information\n",
    "    mi_scores = mutual_info_classif(X, y, random_state=random_state)\n",
    "    importance_results['mutual_information'] = mi_scores\n",
    "    \n",
    "    # 3. F-statistics\n",
    "    f_scores, f_pvalues = f_classif(X, y)\n",
    "    importance_results['f_statistic'] = f_scores\n",
    "    importance_results['f_pvalue'] = f_pvalues\n",
    "    \n",
    "    # 4. Univariate feature selection\n",
    "    selector = SelectKBest(score_func=f_classif, k='all')\n",
    "    selector.fit(X, y)\n",
    "    importance_results['univariate_score'] = selector.scores_\n",
    "    \n",
    "    # 5. Logistic Regression coefficients (with standardized features)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    lr = LogisticRegression(random_state=random_state, max_iter=1000)\n",
    "    lr.fit(X_scaled, y)\n",
    "    importance_results['logistic_coef'] = abs(lr.coef_[0])\n",
    "    \n",
    "    # Normalize importance scores to 0-1 range for comparison\n",
    "    for col in importance_results.columns:\n",
    "        if col not in ['f_pvalue']:\n",
    "            importance_results[f'{col}_normalized'] = (\n",
    "                importance_results[col] / importance_results[col].max()\n",
    "            )\n",
    "    \n",
    "    # Calculate composite importance score\n",
    "    importance_cols = [col for col in importance_results.columns if col.endswith('_normalized')]\n",
    "    importance_results['composite_importance'] = importance_results[importance_cols].mean(axis=1)\n",
    "    \n",
    "    return importance_results\n",
    "\n",
    "# Calculate feature importance\n",
    "importance_results = calculate_feature_importance(X, y)\n",
    "\n",
    "# Sort by composite importance\n",
    "importance_results = importance_results.sort_values('composite_importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "display(importance_results[['random_forest', 'mutual_information', 'f_statistic', 'logistic_coef', 'composite_importance']].head(15))\n",
    "\n",
    "# Visualize feature importance comparison\n",
    "top_features = importance_results.head(15).index.tolist()\n",
    "importance_subset = importance_results.loc[top_features]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Random Forest Importance\n",
    "axes[0, 0].barh(range(len(top_features)), importance_subset['random_forest'])\n",
    "axes[0, 0].set_yticks(range(len(top_features)))\n",
    "axes[0, 0].set_yticklabels(top_features)\n",
    "axes[0, 0].set_title('Random Forest Feature Importance')\n",
    "axes[0, 0].set_xlabel('Importance')\n",
    "\n",
    "# Mutual Information\n",
    "axes[0, 1].barh(range(len(top_features)), importance_subset['mutual_information'])\n",
    "axes[0, 1].set_yticks(range(len(top_features)))\n",
    "axes[0, 1].set_yticklabels(top_features)\n",
    "axes[0, 1].set_title('Mutual Information Scores')\n",
    "axes[0, 1].set_xlabel('MI Score')\n",
    "\n",
    "# F-statistics\n",
    "axes[1, 0].barh(range(len(top_features)), importance_subset['f_statistic'])\n",
    "axes[1, 0].set_yticks(range(len(top_features)))\n",
    "axes[1, 0].set_yticklabels(top_features)\n",
    "axes[1, 0].set_title('F-statistic Scores')\n",
    "axes[1, 0].set_xlabel('F-score')\n",
    "\n",
    "# Composite Importance\n",
    "axes[1, 1].barh(range(len(top_features)), importance_subset['composite_importance'])\n",
    "axes[1, 1].set_yticks(range(len(top_features)))\n",
    "axes[1, 1].set_yticklabels(top_features)\n",
    "axes[1, 1].set_title('Composite Importance Score')\n",
    "axes[1, 1].set_xlabel('Composite Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Importance Comparison (Top 15 Features)', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Interactive feature importance plot\n",
    "fig = go.Figure()\n",
    "\n",
    "methods = ['random_forest', 'mutual_information', 'f_statistic', 'logistic_coef']\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=method.replace('_', ' ').title(),\n",
    "        x=top_features,\n",
    "        y=importance_subset[f'{method}_normalized'],\n",
    "        marker_color=colors[i],\n",
    "        opacity=0.7\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Feature Importance Comparison (Normalized)',\n",
    "    xaxis_title='Features',\n",
    "    yaxis_title='Normalized Importance',\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UNIVARIATE ANALYSIS ===\n",
    "print(\"4. UNIVARIATE ANALYSIS\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "def perform_univariate_analysis(df, target_col='defect', top_n=10):\n",
    "    \"\"\"\n",
    "    Detailed univariate analysis for top features\n",
    "    \"\"\"\n",
    "    # Get top features from importance analysis\n",
    "    top_features = importance_results.head(top_n).index.tolist()\n",
    "    \n",
    "    univariate_stats = {}\n",
    "    \n",
    "    for feature in top_features:\n",
    "        stats_dict = {}\n",
    "        \n",
    "        # Overall statistics\n",
    "        stats_dict['mean'] = df[feature].mean()\n",
    "        stats_dict['std'] = df[feature].std()\n",
    "        stats_dict['median'] = df[feature].median()\n",
    "        stats_dict['min'] = df[feature].min()\n",
    "        stats_dict['max'] = df[feature].max()\n",
    "        stats_dict['skewness'] = df[feature].skew()\n",
    "        stats_dict['kurtosis'] = df[feature].kurtosis()\n",
    "        \n",
    "        # Group statistics\n",
    "        normal_data = df[df[target_col] == 0][feature]\n",
    "        defect_data = df[df[target_col] == 1][feature]\n",
    "        \n",
    "        stats_dict['normal_mean'] = normal_data.mean()\n",
    "        stats_dict['defect_mean'] = defect_data.mean()\n",
    "        stats_dict['normal_std'] = normal_data.std()\n",
    "        stats_dict['defect_std'] = defect_data.std()\n",
    "        \n",
    "        # Statistical tests\n",
    "        # Normality tests\n",
    "        _, normal_shapiro_p = stats.shapiro(normal_data.sample(min(5000, len(normal_data))))\n",
    "        _, defect_shapiro_p = stats.shapiro(defect_data.sample(min(5000, len(defect_data))))\n",
    "        \n",
    "        stats_dict['normal_shapiro_p'] = normal_shapiro_p\n",
    "        stats_dict['defect_shapiro_p'] = defect_shapiro_p\n",
    "        \n",
    "        # Variance test\n",
    "        _, levene_p = stats.levene(normal_data, defect_data)\n",
    "        stats_dict['levene_p'] = levene_p\n",
    "        \n",
    "        # Mean difference test\n",
    "        _, ttest_p = stats.ttest_ind(normal_data, defect_data, equal_var=(levene_p > 0.05))\n",
    "        stats_dict['ttest_p'] = ttest_p\n",
    "        \n",
    "        # Mann-Whitney U test (non-parametric)\n",
    "        _, mannwhitney_p = stats.mannwhitneyu(normal_data, defect_data, alternative='two-sided')\n",
    "        stats_dict['mannwhitney_p'] = mannwhitney_p\n",
    "        \n",
    "        univariate_stats[feature] = stats_dict\n",
    "    \n",
    "    return pd.DataFrame(univariate_stats).T\n",
    "\n",
    "# Perform univariate analysis\n",
    "univariate_stats = perform_univariate_analysis(df, top_n=15)\n",
    "\n",
    "print(\"Univariate Statistics for Top 15 Features:\")\n",
    "display(univariate_stats[['mean', 'std', 'skewness', 'kurtosis', 'normal_mean', 'defect_mean', 'ttest_p', 'mannwhitney_p']].round(4))\n",
    "\n",
    "# Visualize distributions of top features\n",
    "top_univariate_features = univariate_stats.index[:12].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(top_univariate_features):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Create box plots\n",
    "    box_data = [\n",
    "        df[df['defect'] == 0][feature].dropna(),\n",
    "        df[df['defect'] == 1][feature].dropna()\n",
    "    ]\n",
    "    \n",
    "    bp = ax.boxplot(box_data, labels=['Normal', 'Defect'], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    bp['boxes'][1].set_facecolor('lightcoral')\n",
    "    \n",
    "    ax.set_title(f'{feature}\\np-value: {univariate_stats.loc[feature, \"mannwhitney_p\"]:.4f}')\n",
    "    ax.set_ylabel(feature)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Univariate Analysis: Box Plots for Top Features', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Statistical significance summary\n",
    "significant_features = univariate_stats[univariate_stats['mannwhitney_p'] < 0.05]\n",
    "print(f\"\\nStatistically significant features (p < 0.05): {len(significant_features)}\")\n",
    "print(f\"Percentage of significant features: {len(significant_features)/len(univariate_stats)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BIVARIATE ANALYSIS ===\n",
    "print(\"5. BIVARIATE ANALYSIS\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "def perform_bivariate_analysis(df, target_col='defect', top_n=8):\n",
    "    \"\"\"\n",
    "    Bivariate analysis between top features and target\n",
    "    \"\"\"\n",
    "    top_features = importance_results.head(top_n).index.tolist()\n",
    "    \n",
    "    # Create pair plots for top features\n",
    "    plot_df = df[top_features + [target_col]].copy()\n",
    "    plot_df[target_col] = plot_df[target_col].map({0: 'Normal', 1: 'Defect'})\n",
    "    \n",
    "    # Seaborn pairplot\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    g = sns.pairplot(\n",
    "        plot_df, \n",
    "        hue=target_col,\n",
    "        diag_kind='hist',\n",
    "        plot_kws={'alpha': 0.6},\n",
    "        diag_kws={'alpha': 0.7}\n",
    "    )\n",
    "    g.fig.suptitle('Bivariate Analysis: Feature Pairs Colored by Target', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature interaction analysis\n",
    "    interaction_results = {}\n",
    "    \n",
    "    for i, feature1 in enumerate(top_features):\n",
    "        for j, feature2 in enumerate(top_features[i+1:], i+1):\n",
    "            # Calculate interaction effect\n",
    "            normal_data = df[df[target_col] == 'Normal' if target_col in ['Normal', 'Defect'] else df[target_col] == 0]\n",
    "            defect_data = df[df[target_col] == 'Defect' if target_col in ['Normal', 'Defect'] else df[target_col] == 1]\n",
    "            \n",
    "            # Correlation within groups\n",
    "            normal_corr = normal_data[[feature1, feature2]].corr().iloc[0, 1]\n",
    "            defect_corr = defect_data[[feature1, feature2]].corr().iloc[0, 1]\n",
    "            \n",
    "            interaction_results[f\"{feature1}_x_{feature2}\"] = {\n",
    "                'normal_correlation': normal_corr,\n",
    "                'defect_correlation': defect_corr,\n",
    "                'correlation_difference': abs(defect_corr - normal_corr)\n",
    "            }\n",
    "    \n",
    "    return pd.DataFrame(interaction_results).T\n",
    "\n",
    "# Reset target column if changed\n",
    "df['defect'] = y  # Ensure numeric target\n",
    "\n",
    "bivariate_results = perform_bivariate_analysis(df, top_n=6)\n",
    "\n",
    "print(\"Feature Interaction Analysis (Top 6 Features):\")\n",
    "bivariate_sorted = bivariate_results.sort_values('correlation_difference', ascending=False)\n",
    "display(bivariate_sorted.head(10))\n",
    "\n",
    "# Scatter plots for most interesting pairs\n",
    "top_interactions = bivariate_sorted.head(4).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, interaction in enumerate(top_interactions):\n",
    "    feature1, feature2 = interaction.split('_x_')\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Scatter plot colored by target\n",
    "    normal_idx = df['defect'] == 0\n",
    "    defect_idx = df['defect'] == 1\n",
    "    \n",
    "    ax.scatter(df.loc[normal_idx, feature1], df.loc[normal_idx, feature2], \n",
    "              alpha=0.6, label='Normal', color='blue')\n",
    "    ax.scatter(df.loc[defect_idx, feature1], df.loc[defect_idx, feature2], \n",
    "              alpha=0.6, label='Defect', color='red')\n",
    "    \n",
    "    ax.set_xlabel(feature1)\n",
    "    ax.set_ylabel(feature2)\n",
    "    ax.set_title(f'{feature1} vs {feature2}\\nCorr Diff: {bivariate_results.loc[interaction, \"correlation_difference\"]:.3f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Bivariate Analysis: Feature Interactions', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE SELECTION RECOMMENDATIONS ===\n",
    "print(\"6. FEATURE SELECTION RECOMMENDATIONS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "def generate_feature_recommendations(importance_results, correlation_results, univariate_stats, threshold_importance=0.1, threshold_correlation=0.8):\n",
    "    \"\"\"\n",
    "    Generate comprehensive feature selection recommendations\n",
    "    \"\"\"\n",
    "    recommendations = {}\n",
    "    \n",
    "    # 1. High importance features\n",
    "    high_importance = importance_results[importance_results['composite_importance'] > threshold_importance]\n",
    "    recommendations['high_importance'] = high_importance.index.tolist()\n",
    "    \n",
    "    # 2. Statistically significant features\n",
    "    significant_features = univariate_stats[univariate_stats['mannwhitney_p'] < 0.05].index.tolist()\n",
    "    recommendations['statistically_significant'] = significant_features\n",
    "    \n",
    "    # 3. Low correlation features (avoid multicollinearity)\n",
    "    high_corr_pairs = find_high_correlations(correlation_results, threshold=threshold_correlation)\n",
    "    \n",
    "    # For high correlation pairs, keep the one with higher importance\n",
    "    features_to_remove = set()\n",
    "    for _, row in high_corr_pairs.iterrows():\n",
    "        feature1, feature2 = row['feature1'], row['feature2']\n",
    "        if feature1 in importance_results.index and feature2 in importance_results.index:\n",
    "            if importance_results.loc[feature1, 'composite_importance'] > importance_results.loc[feature2, 'composite_importance']:\n",
    "                features_to_remove.add(feature2)\n",
    "            else:\n",
    "                features_to_remove.add(feature1)\n",
    "    \n",
    "    low_corr_features = [f for f in importance_results.index if f not in features_to_remove]\n",
    "    recommendations['low_multicollinearity'] = low_corr_features\n",
    "    \n",
    "    # 4. Combined recommendations\n",
    "    final_features = list(set(recommendations['high_importance']) & \n",
    "                         set(recommendations['statistically_significant']) & \n",
    "                         set(recommendations['low_multicollinearity']))\n",
    "    \n",
    "    recommendations['final_recommended'] = sorted(final_features, \n",
    "                                                 key=lambda x: importance_results.loc[x, 'composite_importance'], \n",
    "                                                 reverse=True)\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations\n",
    "feature_recommendations = generate_feature_recommendations(\n",
    "    importance_results, \n",
    "    pearson_corr, \n",
    "    univariate_stats,\n",
    "    threshold_importance=0.05,\n",
    "    threshold_correlation=0.8\n",
    ")\n",
    "\n",
    "print(\"FEATURE SELECTION RECOMMENDATIONS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for category, features in feature_recommendations.items():\n",
    "    print(f\"\\n{category.replace('_', ' ').title()}: {len(features)} features\")\n",
    "    if len(features) <= 20:  # Show all if reasonable number\n",
    "        for i, feature in enumerate(features[:20], 1):\n",
    "            importance_score = importance_results.loc[feature, 'composite_importance'] if feature in importance_results.index else 'N/A'\n",
    "            print(f\"  {i:2d}. {feature} (importance: {importance_score:.3f})\")\n",
    "    else:  # Show top 20\n",
    "        for i, feature in enumerate(features[:20], 1):\n",
    "            importance_score = importance_results.loc[feature, 'composite_importance'] if feature in importance_results.index else 'N/A'\n",
    "            print(f\"  {i:2d}. {feature} (importance: {importance_score:.3f})\")\n",
    "        print(f\"      ... and {len(features) - 20} more\")\n",
    "\n",
    "# Feature selection performance comparison\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def evaluate_feature_sets(X, y, feature_sets, cv=5):\n",
    "    \"\"\"Evaluate different feature sets using cross-validation\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    for set_name, features in feature_sets.items():\n",
    "        if len(features) > 0:\n",
    "            X_subset = X[features]\n",
    "            scores = cross_val_score(rf, X_subset, y, cv=cv, scoring='roc_auc')\n",
    "            results[set_name] = {\n",
    "                'n_features': len(features),\n",
    "                'mean_auc': scores.mean(),\n",
    "                'std_auc': scores.std(),\n",
    "                'scores': scores\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate feature sets\n",
    "evaluation_sets = {\n",
    "    'all_features': X.columns.tolist(),\n",
    "    'top_10_importance': importance_results.head(10).index.tolist(),\n",
    "    'top_20_importance': importance_results.head(20).index.tolist(),\n",
    "    'final_recommended': feature_recommendations['final_recommended']\n",
    "}\n",
    "\n",
    "evaluation_results = evaluate_feature_sets(X, y, evaluation_sets)\n",
    "\n",
    "print(\"\\n\\nFEATURE SET PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "performance_df = pd.DataFrame({\n",
    "    name: {\n",
    "        'n_features': results['n_features'],\n",
    "        'mean_auc': results['mean_auc'],\n",
    "        'std_auc': results['std_auc']\n",
    "    }\n",
    "    for name, results in evaluation_results.items()\n",
    "}).T\n",
    "\n",
    "performance_df = performance_df.sort_values('mean_auc', ascending=False)\n",
    "display(performance_df)\n",
    "\n",
    "# Visualize performance comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# AUC comparison\n",
    "ax1.bar(range(len(performance_df)), performance_df['mean_auc'], \n",
    "        yerr=performance_df['std_auc'], capsize=5)\n",
    "ax1.set_xticks(range(len(performance_df)))\n",
    "ax1.set_xticklabels(performance_df.index, rotation=45)\n",
    "ax1.set_ylabel('Cross-Validation AUC')\n",
    "ax1.set_title('Feature Set Performance Comparison')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Number of features vs AUC\n",
    "ax2.scatter(performance_df['n_features'], performance_df['mean_auc'], s=100)\n",
    "for idx, row in performance_df.iterrows():\n",
    "    ax2.annotate(idx, (row['n_features'], row['mean_auc']), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "ax2.set_xlabel('Number of Features')\n",
    "ax2.set_ylabel('Cross-Validation AUC')\n",
    "ax2.set_title('Features vs Performance Trade-off')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SUMMARY AND CONCLUSIONS ===\n",
    "print(\"7. SUMMARY AND CONCLUSIONS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"FEATURE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# Key findings\n",
    "print(f\"📊 Dataset Overview:\")\n",
    "print(f\"   • Total features analyzed: {len(X.columns)}\")\n",
    "print(f\"   • Numeric features: {len(numeric_features)}\")\n",
    "print(f\"   • Categorical features: {len(categorical_features)}\")\n",
    "print(f\"   • Class distribution: {(y==0).sum()} normal, {(y==1).sum()} defect\")\n",
    "\n",
    "print(f\"\\n🔍 Distribution Analysis:\")\n",
    "print(f\"   • Features with large effect sizes (|Cohen's d| > 0.5): {len(distribution_results[distribution_results['abs_cohens_d'] > 0.5])}\")\n",
    "print(f\"   • Most discriminative feature: {distribution_results.index[0]} (Cohen's d = {distribution_results.iloc[0]['cohens_d']:.3f})\")\n",
    "\n",
    "print(f\"\\n📈 Correlation Analysis:\")\n",
    "print(f\"   • Features highly correlated with target (|r| > 0.1): {len(target_correlations[target_correlations['abs_pearson'] > 0.1])}\")\n",
    "print(f\"   • Highest target correlation: {target_correlations.index[0]} (r = {target_correlations.iloc[0]['pearson']:.3f})\")\n",
    "print(f\"   • High correlation pairs (potential multicollinearity): {len(high_corr_pairs) if not high_corr_pairs.empty else 0}\")\n",
    "\n",
    "print(f\"\\n🎯 Feature Importance:\")\n",
    "print(f\"   • Top feature by composite importance: {importance_results.index[0]} (score: {importance_results.iloc[0]['composite_importance']:.3f})\")\n",
    "print(f\"   • Features with high importance (score > 0.1): {len(importance_results[importance_results['composite_importance'] > 0.1])}\")\n",
    "\n",
    "print(f\"\\n📊 Statistical Significance:\")\n",
    "print(f\"   • Statistically significant features (p < 0.05): {len(univariate_stats[univariate_stats['mannwhitney_p'] < 0.05])}\")\n",
    "print(f\"   • Features with very high significance (p < 0.001): {len(univariate_stats[univariate_stats['mannwhitney_p'] < 0.001])}\")\n",
    "\n",
    "print(f\"\\n✅ Final Recommendations:\")\n",
    "print(f\"   • Recommended feature set size: {len(feature_recommendations['final_recommended'])}\")\n",
    "print(f\"   • Expected performance with recommended features: {evaluation_results['final_recommended']['mean_auc']:.3f} ± {evaluation_results['final_recommended']['std_auc']:.3f} AUC\")\n",
    "print(f\"   • Performance vs all features: {evaluation_results['final_recommended']['mean_auc'] - evaluation_results['all_features']['mean_auc']:+.3f}\")\n",
    "\n",
    "print(f\"\\n📋 Top 10 Recommended Features:\")\n",
    "for i, feature in enumerate(feature_recommendations['final_recommended'][:10], 1):\n",
    "    importance_score = importance_results.loc[feature, 'composite_importance']\n",
    "    target_corr = target_correlations.loc[feature, 'pearson'] if feature in target_correlations.index else 'N/A'\n",
    "    print(f\"   {i:2d}. {feature}\")\n",
    "    print(f\"       • Importance: {importance_score:.3f}\")\n",
    "    print(f\"       • Target correlation: {target_corr:.3f}\" if target_corr != 'N/A' else \"       • Target correlation: N/A\")\n",
    "\n",
    "print(f\"\\n🎯 Next Steps:\")\n",
    "print(f\"   1. Use recommended feature set for model training\")\n",
    "print(f\"   2. Consider feature engineering for top features\")\n",
    "print(f\"   3. Monitor for concept drift in important features\")\n",
    "print(f\"   4. Investigate domain meaning of top features\")\n",
    "print(f\"   5. Consider interaction terms for bivariate analysis\")\n",
    "print(f\"   6. Validate findings on holdout test set\")\n",
    "print(f\"   7. Document feature importance for model interpretability\")\n",
    "\n",
    "print(f\"\\n📊 Analysis Complete: {len(X.columns)} features analyzed, {len(feature_recommendations['final_recommended'])} recommended for modeling\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}