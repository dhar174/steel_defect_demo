{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis for Steel Casting Defect Prediction\n",
    "\n",
    "**Date:** 2025-01-21  \n",
    "**Author:** Steel Defect Demo Team  \n",
    "**Purpose:** Comprehensive analysis of features for defect prediction\n",
    "\n",
    "## Overview\n",
    "This notebook provides a comprehensive analysis of the steel casting dataset, focusing on:\n",
    "- Feature distributions comparing normal vs defect casts\n",
    "- Correlation analysis with visualization\n",
    "- Feature importance ranking using multiple methods\n",
    "- Univariate and bivariate feature analysis\n",
    "- Feature selection recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Analysis for Steel Casting Defect Prediction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(\"Feature Analysis Notebook - Steel Casting Defect Prediction\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample data\n",
    "df = pd.read_csv('../data/examples/steel_defect_sample.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target distribution:\\n{df['defect'].value_counts()}\")\n",
    "print(f\"Target proportions:\\n{df['defect'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"\\nDataset columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "missing_data = df.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    print(f\"\\nMissing values:\\n{missing_data[missing_data > 0]}\")\n",
    "else:\n",
    "    print(\"\\nNo missing values found\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('defect', axis=1)\n",
    "y = df['defect']\n",
    "\n",
    "print(f\"\\nFeatures ({len(X.columns)}): {X.columns.tolist()}\")\n",
    "print(f\"Target variable: defect (0: Normal, 1: Defect)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE DISTRIBUTION ANALYSIS ===\n",
    "print(\"1. FEATURE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def analyze_feature_distributions(df, target_col='defect'):\n",
    "    \"\"\"\n",
    "    Analyze feature distributions comparing normal vs defect casts\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Separate normal and defect samples\n",
    "    normal_data = df[df[target_col] == 0]\n",
    "    defect_data = df[df[target_col] == 1]\n",
    "    \n",
    "    feature_cols = df.columns.drop(target_col).tolist()\n",
    "    \n",
    "    for feature in feature_cols:\n",
    "        # Statistical tests\n",
    "        try:\n",
    "            stat_test = stats.mannwhitneyu(\n",
    "                normal_data[feature].dropna(),\n",
    "                defect_data[feature].dropna(),\n",
    "                alternative='two-sided'\n",
    "            )\n",
    "        except:\n",
    "            stat_test = type('obj', (object,), {'statistic': 0, 'pvalue': 1})\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        normal_mean = normal_data[feature].mean()\n",
    "        defect_mean = defect_data[feature].mean()\n",
    "        normal_std = normal_data[feature].std()\n",
    "        defect_std = defect_data[feature].std()\n",
    "        \n",
    "        pooled_std = np.sqrt(\n",
    "            ((len(normal_data[feature]) - 1) * normal_std**2 +\n",
    "             (len(defect_data[feature]) - 1) * defect_std**2) /\n",
    "            (len(normal_data[feature]) + len(defect_data[feature]) - 2)\n",
    "        )\n",
    "        cohens_d = (defect_mean - normal_mean) / pooled_std if pooled_std > 0 else 0\n",
    "        \n",
    "        results[feature] = {\n",
    "            'normal_mean': normal_mean,\n",
    "            'defect_mean': defect_mean,\n",
    "            'normal_std': normal_std,\n",
    "            'defect_std': defect_std,\n",
    "            'mannwhitney_statistic': stat_test.statistic,\n",
    "            'mannwhitney_pvalue': stat_test.pvalue,\n",
    "            'cohens_d': cohens_d,\n",
    "            'abs_cohens_d': abs(cohens_d)\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "# Perform distribution analysis\n",
    "distribution_results = analyze_feature_distributions(df)\n",
    "distribution_results = distribution_results.sort_values('abs_cohens_d', ascending=False)\n",
    "\n",
    "print(\"Feature distribution analysis results:\")\n",
    "print(distribution_results.round(4))\n",
    "\n",
    "# Visualize top discriminative features\n",
    "top_features = distribution_results.head(8).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(top_features):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot distributions\n",
    "    normal_data = df[df['defect'] == 0][feature]\n",
    "    defect_data = df[df['defect'] == 1][feature]\n",
    "    \n",
    "    ax.hist(normal_data, alpha=0.7, label='Normal', bins=30, density=True, color='blue')\n",
    "    ax.hist(defect_data, alpha=0.7, label='Defect', bins=30, density=True, color='red')\n",
    "    \n",
    "    cohens_d = distribution_results.loc[feature, 'cohens_d']\n",
    "    ax.set_title(f'{feature}\\nCohen\\'s d = {cohens_d:.3f}')\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Distributions: Normal vs Defect Casts', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CORRELATION ANALYSIS ===\n",
    "print(\"2. CORRELATION ANALYSIS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "target_correlations = correlation_matrix['defect'].drop('defect').abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Features most correlated with defect:\")\n",
    "print(target_correlations.round(4))\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    fmt='.2f',\n",
    "    cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "plt.title('Feature Correlation Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# High correlation pairs (potential multicollinearity)\n",
    "def find_high_correlations(corr_matrix, threshold=0.8):\n",
    "    \"\"\"Find pairs of features with high correlation\"\"\"\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > threshold:\n",
    "                high_corr_pairs.append({\n",
    "                    'feature1': corr_matrix.columns[i],\n",
    "                    'feature2': corr_matrix.columns[j],\n",
    "                    'correlation': corr_val\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(high_corr_pairs)\n",
    "\n",
    "high_corr_pairs = find_high_correlations(correlation_matrix, threshold=0.7)\n",
    "if not high_corr_pairs.empty:\n",
    "    print(f\"\\nHigh correlation pairs (|r| > 0.7): {len(high_corr_pairs)}\")\n",
    "    print(high_corr_pairs)\n",
    "else:\n",
    "    print(\"\\nNo high correlation pairs found (|r| > 0.7)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE IMPORTANCE ANALYSIS ===\n",
    "print(\"3. FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "def calculate_feature_importance(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Calculate feature importance using multiple methods\n",
    "    \"\"\"\n",
    "    importance_results = pd.DataFrame(index=X.columns)\n",
    "    \n",
    "    # 1. Random Forest Importance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    rf.fit(X, y)\n",
    "    importance_results['random_forest'] = rf.feature_importances_\n",
    "    \n",
    "    # 2. Mutual Information\n",
    "    mi_scores = mutual_info_classif(X, y, random_state=random_state)\n",
    "    importance_results['mutual_information'] = mi_scores\n",
    "    \n",
    "    # 3. F-statistics\n",
    "    f_scores, f_pvalues = f_classif(X, y)\n",
    "    importance_results['f_statistic'] = f_scores\n",
    "    importance_results['f_pvalue'] = f_pvalues\n",
    "    \n",
    "    # 4. Correlation with target\n",
    "    importance_results['correlation'] = abs(X.corrwith(y))\n",
    "    \n",
    "    # Normalize importance scores to 0-1 range for comparison\n",
    "    for col in ['random_forest', 'mutual_information', 'f_statistic', 'correlation']:\n",
    "        if col in importance_results.columns:\n",
    "            max_val = importance_results[col].max()\n",
    "            if max_val > 0:\n",
    "                importance_results[f'{col}_normalized'] = importance_results[col] / max_val\n",
    "    \n",
    "    # Calculate composite importance score\n",
    "    importance_cols = [col for col in importance_results.columns if col.endswith('_normalized')]\n",
    "    importance_results['composite_importance'] = importance_results[importance_cols].mean(axis=1)\n",
    "    \n",
    "    return importance_results\n",
    "\n",
    "# Calculate feature importance\n",
    "importance_results = calculate_feature_importance(X, y)\n",
    "importance_results = importance_results.sort_values('composite_importance', ascending=False)\n",
    "\n",
    "print(\"Feature importance results:\")\n",
    "print(importance_results[['random_forest', 'mutual_information', 'correlation', 'composite_importance']].round(4))\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Random Forest Importance\n",
    "axes[0, 0].barh(range(len(X.columns)), importance_results['random_forest'])\n",
    "axes[0, 0].set_yticks(range(len(X.columns)))\n",
    "axes[0, 0].set_yticklabels(importance_results.index)\n",
    "axes[0, 0].set_title('Random Forest Feature Importance')\n",
    "axes[0, 0].set_xlabel('Importance')\n",
    "\n",
    "# Mutual Information\n",
    "axes[0, 1].barh(range(len(X.columns)), importance_results['mutual_information'])\n",
    "axes[0, 1].set_yticks(range(len(X.columns)))\n",
    "axes[0, 1].set_yticklabels(importance_results.index)\n",
    "axes[0, 1].set_title('Mutual Information Scores')\n",
    "axes[0, 1].set_xlabel('MI Score')\n",
    "\n",
    "# Correlation\n",
    "axes[1, 0].barh(range(len(X.columns)), importance_results['correlation'])\n",
    "axes[1, 0].set_yticks(range(len(X.columns)))\n",
    "axes[1, 0].set_yticklabels(importance_results.index)\n",
    "axes[1, 0].set_title('Correlation with Target')\n",
    "axes[1, 0].set_xlabel('Absolute Correlation')\n",
    "\n",
    "# Composite Importance\n",
    "axes[1, 1].barh(range(len(X.columns)), importance_results['composite_importance'])\n",
    "axes[1, 1].set_yticks(range(len(X.columns)))\n",
    "axes[1, 1].set_yticklabels(importance_results.index)\n",
    "axes[1, 1].set_title('Composite Importance Score')\n",
    "axes[1, 1].set_xlabel('Composite Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Importance Comparison', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE SELECTION RECOMMENDATIONS ===\n",
    "print(\"4. FEATURE SELECTION RECOMMENDATIONS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Performance comparison of different feature sets\n",
    "def evaluate_feature_sets(X, y, feature_sets, cv=5):\n",
    "    \"\"\"Evaluate different feature sets using cross-validation\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    for set_name, features in feature_sets.items():\n",
    "        if len(features) > 0 and all(f in X.columns for f in features):\n",
    "            X_subset = X[features]\n",
    "            scores = cross_val_score(rf, X_subset, y, cv=cv, scoring='roc_auc')\n",
    "            results[set_name] = {\n",
    "                'n_features': len(features),\n",
    "                'mean_auc': scores.mean(),\n",
    "                'std_auc': scores.std(),\n",
    "                'features': features\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define feature sets to evaluate\n",
    "feature_sets = {\n",
    "    'all_features': X.columns.tolist(),\n",
    "    'top_5_importance': importance_results.head(5).index.tolist(),\n",
    "    'top_8_importance': importance_results.head(8).index.tolist(),\n",
    "    'top_3_correlation': target_correlations.head(3).index.tolist(),\n",
    "    'top_5_correlation': target_correlations.head(5).index.tolist()\n",
    "}\n",
    "\n",
    "# Evaluate feature sets\n",
    "evaluation_results = evaluate_feature_sets(X, y, feature_sets)\n",
    "\n",
    "print(\"Feature Set Performance Comparison:\")\n",
    "performance_df = pd.DataFrame({\n",
    "    name: {\n",
    "        'n_features': results['n_features'],\n",
    "        'mean_auc': results['mean_auc'],\n",
    "        'std_auc': results['std_auc']\n",
    "    }\n",
    "    for name, results in evaluation_results.items()\n",
    "}).T\n",
    "\n",
    "performance_df = performance_df.sort_values('mean_auc', ascending=False)\n",
    "print(performance_df.round(4))\n",
    "\n",
    "# Visualize performance comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# AUC comparison\n",
    "ax1.bar(range(len(performance_df)), performance_df['mean_auc'], \n",
    "        yerr=performance_df['std_auc'], capsize=5)\n",
    "ax1.set_xticks(range(len(performance_df)))\n",
    "ax1.set_xticklabels(performance_df.index, rotation=45)\n",
    "ax1.set_ylabel('Cross-Validation AUC')\n",
    "ax1.set_title('Feature Set Performance Comparison')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Number of features vs AUC\n",
    "ax2.scatter(performance_df['n_features'], performance_df['mean_auc'], s=100)\n",
    "for idx, row in performance_df.iterrows():\n",
    "    ax2.annotate(idx, (row['n_features'], row['mean_auc']), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "ax2.set_xlabel('Number of Features')\n",
    "ax2.set_ylabel('Cross-Validation AUC')\n",
    "ax2.set_title('Features vs Performance Trade-off')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE SELECTION RECOMMENDATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_set = performance_df.index[0]\n",
    "print(f\"\\n🏆 Best performing feature set: {best_set}\")\n",
    "print(f\"   • Number of features: {evaluation_results[best_set]['n_features']}\")\n",
    "print(f\"   • Cross-validation AUC: {evaluation_results[best_set]['mean_auc']:.4f} ± {evaluation_results[best_set]['std_auc']:.4f}\")\n",
    "print(f\"   • Features: {evaluation_results[best_set]['features']}\")\n",
    "\n",
    "print(f\"\\n📊 Top features by different methods:\")\n",
    "print(f\"   • By composite importance: {importance_results.head(5).index.tolist()}\")\n",
    "print(f\"   • By correlation with target: {target_correlations.head(5).index.tolist()}\")\n",
    "print(f\"   • By effect size (Cohen's d): {distribution_results.head(5).index.tolist()}\")\n",
    "\n",
    "print(f\"\\n🎯 Final recommendations:\")\n",
    "print(f\"   1. Use top 5-8 features based on composite importance for best balance\")\n",
    "print(f\"   2. Monitor high-correlation features for multicollinearity\")\n",
    "print(f\"   3. Consider domain expertise for final feature selection\")\n",
    "print(f\"   4. Validate on holdout test set before deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SUMMARY AND EXPORT ===\n",
    "print(\"5. SUMMARY AND EXPORT\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*20)\n",
    "print(f\"📊 Dataset: {df.shape[0]} samples, {len(X.columns)} features\")\n",
    "print(f\"📈 Class distribution: {(y==0).sum()} normal ({(y==0).sum()/len(y)*100:.1f}%), {(y==1).sum()} defect ({(y==1).sum()/len(y)*100:.1f}%)\")\n",
    "print(f\"🔍 Best feature by importance: {importance_results.index[0]} (score: {importance_results.iloc[0]['composite_importance']:.3f})\")\n",
    "print(f\"📊 Best feature by correlation: {target_correlations.index[0]} (r = {target_correlations.iloc[0]:.3f})\")\n",
    "print(f\"⚡ Best feature by effect size: {distribution_results.index[0]} (Cohen's d = {distribution_results.iloc[0]['cohens_d']:.3f})\")\n",
    "\n",
    "# Export results\n",
    "print(f\"\\n💾 Exporting analysis results...\")\n",
    "\n",
    "# Create results directory\n",
    "import os\n",
    "results_dir = '../results/feature_analysis'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Export feature importance results\n",
    "importance_results.to_csv(f'{results_dir}/feature_importance.csv')\n",
    "print(f\"   ✅ Feature importance saved to {results_dir}/feature_importance.csv\")\n",
    "\n",
    "# Export distribution analysis\n",
    "distribution_results.to_csv(f'{results_dir}/distribution_analysis.csv')\n",
    "print(f\"   ✅ Distribution analysis saved to {results_dir}/distribution_analysis.csv\")\n",
    "\n",
    "# Export correlation matrix\n",
    "correlation_matrix.to_csv(f'{results_dir}/correlation_matrix.csv')\n",
    "print(f\"   ✅ Correlation matrix saved to {results_dir}/correlation_matrix.csv\")\n",
    "\n",
    "# Export performance comparison\n",
    "performance_df.to_csv(f'{results_dir}/feature_set_performance.csv')\n",
    "print(f\"   ✅ Feature set performance saved to {results_dir}/feature_set_performance.csv\")\n",
    "\n",
    "# Export recommended features\n",
    "recommended_features = {\n",
    "    'top_5_importance': importance_results.head(5).index.tolist(),\n",
    "    'top_8_importance': importance_results.head(8).index.tolist(),\n",
    "    'top_5_correlation': target_correlations.head(5).index.tolist(),\n",
    "    'best_performing_set': evaluation_results[best_set]['features']\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{results_dir}/recommended_features.json', 'w') as f:\n",
    "    json.dump(recommended_features, f, indent=2)\n",
    "print(f\"   ✅ Recommended features saved to {results_dir}/recommended_features.json\")\n",
    "\n",
    "print(f\"\\n🎉 Feature analysis complete! Results exported to {results_dir}/\")\n",
    "print(f\"\\n📋 Key takeaways:\")\n",
    "print(f\"   • Use {len(evaluation_results[best_set]['features'])} features for optimal performance\")\n",
    "print(f\"   • Expected AUC: {evaluation_results[best_set]['mean_auc']:.3f}\")\n",
    "print(f\"   • Focus on: {', '.join(importance_results.head(3).index.tolist())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}