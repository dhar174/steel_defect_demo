{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 Stage 4: Data Exploration Framework - Comprehensive Steel Defect Analysis\n",
    "\n",
    "This notebook implements a comprehensive data exploration framework for steel casting defect prediction. It serves as the foundation for understanding synthetic data patterns, validating data generation logic, and informing feature engineering decisions for both baseline XGBoost and LSTM models.\n",
    "\n",
    "## Analysis Components:\n",
    "1. **Sensor Time Series Visualization** - Normal vs Defect Cast Comparison\n",
    "2. **Statistical Distribution Analysis** - Defect Class Stratification \n",
    "3. **Correlation Matrices Between Sensors** - Cross-sensor and time-lagged correlations\n",
    "4. **Defect Labeling Validation** - Logic verification and edge case analysis\n",
    "5. **Data Quality Assessment** - Missing values, consistency, and realism checks\n",
    "6. **Interactive Dashboards** - Multi-sensor exploration with filtering\n",
    "7. **Feature Engineering Recommendations** - Insights for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup plotting\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synthetic_data():\n",
    "    \"\"\"Load all synthetic steel casting data\"\"\"\n",
    "    data_path = Path('../data')\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(data_path / 'synthetic/dataset_metadata.json', 'r') as f:\n",
    "        dataset_info = json.load(f)\n",
    "    \n",
    "    metadata_df = pd.DataFrame(dataset_info['cast_metadata'])\n",
    "    \n",
    "    # Load time series data for sample casts\n",
    "    raw_files = list((data_path / 'raw').glob('cast_timeseries_*.parquet'))\n",
    "    \n",
    "    print(f\"Found {len(raw_files)} time series files\")\n",
    "    print(f\"Loading first 50 casts for exploration...\")\n",
    "    \n",
    "    # Load subset for initial exploration (to manage memory)\n",
    "    sample_data = []\n",
    "    for i, file_path in enumerate(raw_files[:50]):\n",
    "        df = pd.read_parquet(file_path)\n",
    "        df['cast_id'] = f\"cast_{i+1:04d}\"\n",
    "        sample_data.append(df)\n",
    "    \n",
    "    combined_data = pd.concat(sample_data, ignore_index=False)\n",
    "    \n",
    "    return combined_data, metadata_df, dataset_info\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    time_series_data, metadata, dataset_info = load_synthetic_data()\n",
    "    print(\"âœ“ Data loaded successfully!\")\n",
    "    print(f\"Dataset info: {dataset_info['dataset_info']['total_casts']} total casts\")\n",
    "    print(f\"Defect rate: {dataset_info['dataset_info']['defect_rate']:.2%}\")\nexcept Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please ensure synthetic data has been generated first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview and Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic dataset information\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Time series data shape: {time_series_data.shape}\")\n",
    "print(f\"Metadata shape: {metadata.shape}\")\n",
    "print(f\"Time range: {time_series_data.index.min()} to {time_series_data.index.max()}\")\n",
    "print(f\"Sensor columns: {list(time_series_data.columns)}\")\n",
    "\n",
    "print(\"\\n=== DEFECT STATISTICS ===\")\n",
    "defect_counts = metadata['defect_label'].value_counts()\n",
    "print(f\"Good casts: {defect_counts[0]} ({defect_counts[0]/len(metadata):.1%})\")\n",
    "print(f\"Defect casts: {defect_counts[1]} ({defect_counts[1]/len(metadata):.1%})\")\n",
    "\n",
    "print(\"\\n=== SENSOR STATISTICS ===\")\n",
    "sensor_stats = time_series_data.describe()\n",
    "display(sensor_stats)\n",
    "\n",
    "print(\"\\n=== STEEL GRADE DISTRIBUTION ===\")\n",
    "grade_dist = metadata['steel_grade'].value_counts()\n",
    "print(grade_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sensor Time Series Visualization - Normal vs Defect Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample good and defect casts for comparison\n",
    "good_casts = metadata[metadata['defect_label'] == 0]['cast_id'].head(3).tolist()\n",
    "defect_casts = metadata[metadata['defect_label'] == 1]['cast_id'].head(3).tolist()\n",
    "\n",
    "print(f\"Analyzing good casts: {good_casts}\")\n",
    "print(f\"Analyzing defect casts: {defect_casts}\")\n",
    "\n",
    "def plot_sensor_comparison(sensor_name, time_series_data, good_casts, defect_casts):\n",
    "    \"\"\"Plot sensor time series comparing good vs defect casts\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    \n",
    "    # Plot good casts\n",
    "    for cast_id in good_casts:\n",
    "        cast_data = time_series_data[time_series_data['cast_id'] == cast_id]\n",
    "        if not cast_data.empty:\n",
    "            ax1.plot(cast_data.index, cast_data[sensor_name], alpha=0.7, label=f'Good {cast_id}')\n",
    "    \n",
    "    ax1.set_title(f'{sensor_name.replace(\"_\", \" \").title()} - Good Casts')\n",
    "    ax1.set_ylabel('Sensor Value')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot defect casts\n",
    "    for cast_id in defect_casts:\n",
    "        cast_data = time_series_data[time_series_data['cast_id'] == cast_id]\n",
    "        if not cast_data.empty:\n",
    "                ax2.plot(cast_data.index, cast_data[sensor_name], alpha=0.7, label=f'Defect {cast_id}', color=DEFECT_CAST_COLOR)\n",
    "    \n",
    "    ax2.set_title(f'{sensor_name.replace(\"_\", \" \").title()} - Defect Casts')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_ylabel('Sensor Value')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot all sensors\n",
    "sensor_columns = [col for col in time_series_data.columns if col != 'cast_id']\n",
    "for sensor in sensor_columns:\n",
    "    plot_sensor_comparison(sensor, time_series_data, good_casts, defect_casts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Sensor Dashboard and Pattern Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive multi-sensor dashboard\n",
    "def create_interactive_dashboard(cast_id):\n",
    "    \"\"\"Create interactive Plotly dashboard for a single cast\"\"\"\n",
    "    cast_data = time_series_data[time_series_data['cast_id'] == cast_id]\n",
    "    cast_meta = metadata[metadata['cast_id'] == cast_id].iloc[0]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=('Casting Speed', 'Mold Temperature', 'Mold Level', \n",
    "                       'Cooling Water Flow', 'Superheat', 'Process Summary'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"type\": \"table\"}]]\n",
    "    )\n",
    "    \n",
    "    sensors = ['casting_speed', 'mold_temperature', 'mold_level', 'cooling_water_flow', 'superheat']\n",
    "    positions = [(1,1), (1,2), (2,1), (2,2), (3,1)]\n",
    "    \n",
    "    for sensor, (row, col) in zip(sensors, positions):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=cast_data.index, y=cast_data[sensor], \n",
    "                      name=sensor.replace('_', ' ').title(),\n",
    "                      line=dict(color='red' if cast_meta['defect_label'] else 'blue')),\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    # Add summary table\n",
    "    summary_data = [\n",
    "        ['Cast ID', cast_id],\n",
    "        ['Defect Status', 'DEFECT' if cast_meta['defect_label'] else 'GOOD'],\n",
    "        ['Steel Grade', cast_meta['steel_grade']],\n",
    "        ['Trigger Events', ', '.join(cast_meta['defect_trigger_events']) if cast_meta['defect_trigger_events'] else 'None'],\n",
    "        ['Avg Speed', f\"{cast_meta['process_summary']['avg_casting_speed']:.2f}\"],\n",
    "        ['Avg Temperature', f\"{cast_meta['process_summary']['avg_mold_temperature']:.1f}\"]\n",
    "    ]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Table(\n",
    "            header=dict(values=['Metric', 'Value']),\n",
    "            cells=dict(values=list(zip(*summary_data)))\n",
    "        ),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"Cast {cast_id} - {'DEFECT' if cast_meta['defect_label'] else 'GOOD'} Analysis\",\n",
    "        height=800,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Display dashboards for sample casts\n",
    "print(\"Interactive Dashboard Examples:\")\n",
    "for cast_id in good_casts[:1] + defect_casts[:1]:\n",
    "    if cast_id in time_series_data['cast_id'].values:\n",
    "        dashboard = create_interactive_dashboard(cast_id)\n",
    "        dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Distribution Analysis with Defect Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for statistical analysis\n",
    "def prepare_statistical_data():\n",
    "    \"\"\"Prepare aggregated statistics for each cast\"\"\"\n",
    "    stats_data = []\n",
    "    \n",
    "    for cast_id in metadata['cast_id']:\n",
    "        cast_data = time_series_data[time_series_data['cast_id'] == cast_id]\n",
    "        cast_meta = metadata[metadata['cast_id'] == cast_id].iloc[0]\n",
    "        \n",
    "        if not cast_data.empty:\n",
    "            cast_stats = {\n",
    "                'cast_id': cast_id,\n",
    "                'defect_label': cast_meta['defect_label'],\n",
    "                'steel_grade': cast_meta['steel_grade']\n",
    "            }\n",
    "            \n",
    "            # Calculate statistics for each sensor\n",
    "            for sensor in sensor_columns:\n",
    "                cast_stats[f'{sensor}_mean'] = cast_data[sensor].mean()\n",
    "                cast_stats[f'{sensor}_std'] = cast_data[sensor].std()\n",
    "                cast_stats[f'{sensor}_min'] = cast_data[sensor].min()\n",
    "                cast_stats[f'{sensor}_max'] = cast_data[sensor].max()\n",
    "                cast_stats[f'{sensor}_range'] = cast_data[sensor].max() - cast_data[sensor].min()\n",
    "            \n",
    "            stats_data.append(cast_stats)\n",
    "    \n",
    "    return pd.DataFrame(stats_data)\n",
    "\n",
    "stats_df = prepare_statistical_data()\n",
    "\n",
    "# Create distribution comparison plots\n",
    "def plot_distribution_comparison(sensor_name, stats_df):\n",
    "    \"\"\"Plot distribution comparison between good and defect casts\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'{sensor_name.replace(\"_\", \" \").title()} - Distribution Analysis')\n",
    "    \n",
    "    # Mean values\n",
    "    good_mean = stats_df[stats_df['defect_label'] == 0][f'{sensor_name}_mean']\n",
    "    defect_mean = stats_df[stats_df['defect_label'] == 1][f'{sensor_name}_mean']\n",
    "    \n",
    "    axes[0,0].hist([good_mean, defect_mean], bins=20, alpha=0.7, \n",
    "                   label=['Good', 'Defect'], color=['blue', 'red'])\n",
    "    axes[0,0].set_title('Mean Values Distribution')\n",
    "    axes[0,0].set_xlabel('Mean Value')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Standard deviation\n",
    "    good_std = stats_df[stats_df['defect_label'] == 0][f'{sensor_name}_std']\n",
    "    defect_std = stats_df[stats_df['defect_label'] == 1][f'{sensor_name}_std']\n",
    "    \n",
    "    axes[0,1].hist([good_std, defect_std], bins=20, alpha=0.7, \n",
    "                   label=['Good', 'Defect'], color=['blue', 'red'])\n",
    "    axes[0,1].set_title('Standard Deviation Distribution')\n",
    "    axes[0,1].set_xlabel('Standard Deviation')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Box plots for mean\n",
    "    box_data = [good_mean, defect_mean]\n",
    "    axes[1,0].boxplot(box_data, labels=['Good', 'Defect'])\n",
    "    axes[1,0].set_title('Mean Values Box Plot')\n",
    "    axes[1,0].set_ylabel('Mean Value')\n",
    "    \n",
    "    # Box plots for range\n",
    "    good_range = stats_df[stats_df['defect_label'] == 0][f'{sensor_name}_range']\n",
    "    defect_range = stats_df[stats_df['defect_label'] == 1][f'{sensor_name}_range']\n",
    "    box_data_range = [good_range, defect_range]\n",
    "    axes[1,1].boxplot(box_data_range, labels=['Good', 'Defect'])\n",
    "    axes[1,1].set_title('Range Values Box Plot')\n",
    "    axes[1,1].set_ylabel('Range Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Perform statistical tests\n",
    "    ks_stat, ks_p = stats.ks_2samp(good_mean, defect_mean)\n",
    "    t_stat, t_p = stats.ttest_ind(good_mean, defect_mean)\n",
    "    \n",
    "    print(f\"\\n{sensor_name.replace('_', ' ').title()} Statistical Tests:\")\n",
    "    print(f\"Kolmogorov-Smirnov test: statistic={ks_stat:.4f}, p-value={ks_p:.4f}\")\n",
    "    print(f\"T-test: statistic={t_stat:.4f}, p-value={t_p:.4f}\")\n",
    "    print(f\"Good casts mean: {good_mean.mean():.3f} Â± {good_mean.std():.3f}\")\n",
    "    print(f\"Defect casts mean: {defect_mean.mean():.3f} Â± {defect_mean.std():.3f}\")\n",
    "\n",
    "# Analyze each sensor\n",
    "for sensor in sensor_columns:\n",
    "    plot_distribution_comparison(sensor, stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Sensor Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations for good and defect casts separately\n",
    "good_stats = stats_df[stats_df['defect_label'] == 0]\n",
    "defect_stats = stats_df[stats_df['defect_label'] == 1]\n",
    "\n",
    "# Create correlation matrices\n",
    "mean_columns = [col for col in stats_df.columns if col.endswith('_mean')]\n",
    "std_columns = [col for col in stats_df.columns if col.endswith('_std')]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Good casts - mean correlations\n",
    "good_corr_mean = good_stats[mean_columns].corr()\n",
    "sns.heatmap(good_corr_mean, annot=True, cmap='coolwarm', center=0, \n",
    "            ax=axes[0,0], fmt='.2f')\n",
    "axes[0,0].set_title('Good Casts - Mean Values Correlation')\n",
    "axes[0,0].set_xticklabels([col.replace('_mean', '').replace('_', ' ').title() for col in mean_columns])\n",
    "axes[0,0].set_yticklabels([col.replace('_mean', '').replace('_', ' ').title() for col in mean_columns])\n",
    "\n",
    "# Defect casts - mean correlations\n",
    "defect_corr_mean = defect_stats[mean_columns].corr()\n",
    "sns.heatmap(defect_corr_mean, annot=True, cmap='coolwarm', center=0, \n",
    "            ax=axes[0,1], fmt='.2f')\n",
    "axes[0,1].set_title('Defect Casts - Mean Values Correlation')\n",
    "axes[0,1].set_xticklabels([col.replace('_mean', '').replace('_', ' ').title() for col in mean_columns])\n",
    "axes[0,1].set_yticklabels([col.replace('_mean', '').replace('_', ' ').title() for col in mean_columns])\n",
    "\n",
    "# Good casts - std correlations\n",
    "good_corr_std = good_stats[std_columns].corr()\n",
    "sns.heatmap(good_corr_std, annot=True, cmap='coolwarm', center=0, \n",
    "            ax=axes[1,0], fmt='.2f')\n",
    "axes[1,0].set_title('Good Casts - Standard Deviation Correlation')\n",
    "axes[1,0].set_xticklabels([col.replace('_std', '').replace('_', ' ').title() for col in std_columns])\n",
    "axes[1,0].set_yticklabels([col.replace('_std', '').replace('_', ' ').title() for col in std_columns])\n",
    "\n",
    "# Defect casts - std correlations\n",
    "defect_corr_std = defect_stats[std_columns].corr()\n",
    "sns.heatmap(defect_corr_std, annot=True, cmap='coolwarm', center=0, \n",
    "            ax=axes[1,1], fmt='.2f')\n",
    "axes[1,1].set_title('Defect Casts - Standard Deviation Correlation')\n",
    "axes[1,1].set_xticklabels([col.replace('_std', '').replace('_', ' ').title() for col in std_columns])\n",
    "axes[1,1].set_yticklabels([col.replace('_std', '').replace('_', ' ').title() for col in std_columns])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze correlation differences\n",
    "print(\"\\n=== CORRELATION ANALYSIS INSIGHTS ===\")\n",
    "corr_diff = defect_corr_mean - good_corr_mean\n",
    "print(\"\\nLargest correlation differences (Defect - Good):\")\n",
    "corr_diff_flat = corr_diff.values.flatten()\n",
    "indices = np.argsort(np.abs(corr_diff_flat))[-10:]\n",
    "for idx in reversed(indices):\n",
    "    i, j = divmod(idx, corr_diff.shape[1])\n",
    "    if i != j and not np.isnan(corr_diff_flat[idx]):\n",
    "        sensor1 = mean_columns[i].replace('_mean', '')\n",
    "        sensor2 = mean_columns[j].replace('_mean', '')\n",
    "        print(f\"{sensor1} - {sensor2}: {corr_diff_flat[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Defect Labeling Validation and Trigger Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze defect trigger events\n",
    "print(\"=== DEFECT TRIGGER ANALYSIS ===\")\n",
    "\n",
    "# Collect all trigger events\n",
    "all_triggers = []\n",
    "for triggers in metadata['defect_trigger_events']:\n",
    "    all_triggers.extend(triggers)\n",
    "\n",
    "trigger_counts = pd.Series(all_triggers).value_counts()\n",
    "print(\"\\nTrigger event frequency:\")\n",
    "for trigger, count in trigger_counts.items():\n",
    "    print(f\"{trigger}: {count} occurrences\")\n",
    "\n",
    "# Analyze defect probability by trigger presence\n",
    "trigger_types = ['prolonged_mold_level_deviation', 'rapid_temperature_drop', 'high_speed_with_low_superheat']\n",
    "\n",
    "trigger_analysis = []\n",
    "for trigger_type in trigger_types:\n",
    "    has_trigger = metadata['defect_trigger_events'].apply(lambda x: trigger_type in x)\n",
    "    defect_rate_with_trigger = metadata[has_trigger]['defect_label'].mean()\n",
    "    defect_rate_without_trigger = metadata[~has_trigger]['defect_label'].mean()\n",
    "    \n",
    "    trigger_analysis.append({\n",
    "        'trigger': trigger_type,\n",
    "        'count_with_trigger': has_trigger.sum(),\n",
    "        'defect_rate_with': defect_rate_with_trigger,\n",
    "        'defect_rate_without': defect_rate_without_trigger,\n",
    "        'relative_risk': defect_rate_with_trigger / defect_rate_without_trigger if defect_rate_without_trigger > 0 else float('inf')\n",
    "    })\n",
    "\n",
    "trigger_df = pd.DataFrame(trigger_analysis)\n",
    "print(\"\\nTrigger event analysis:\")\n",
    "display(trigger_df)\n",
    "\n",
    "# Visualize trigger impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Trigger frequency\n",
    "trigger_df.plot(x='trigger', y='count_with_trigger', kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Trigger Event Frequency')\n",
    "axes[0].set_xlabel('Trigger Type')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Defect rates comparison\n",
    "x_pos = np.arange(len(trigger_types))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x_pos - width/2, trigger_df['defect_rate_with'], width, \n",
    "           label='With Trigger', alpha=0.8)\n",
    "axes[1].bar(x_pos + width/2, trigger_df['defect_rate_without'], width, \n",
    "           label='Without Trigger', alpha=0.8)\n",
    "\n",
    "axes[1].set_title('Defect Rates by Trigger Presence')\n",
    "axes[1].set_xlabel('Trigger Type')\n",
    "axes[1].set_ylabel('Defect Rate')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels([t.replace('_', ' ').title() for t in trigger_types], rotation=45)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Domain-specific validation\n",
    "print(\"\\n=== DOMAIN-SPECIFIC VALIDATION ===\")\n",
    "print(\"\\nValidating defect logic against steel casting domain knowledge:\")\n",
    "print(\"1. Prolonged mold level deviation - Expected to cause surface defects âœ“\")\n",
    "print(\"2. Rapid temperature drop - Can lead to thermal stress and cracking âœ“\")\n",
    "print(\"3. High speed with low superheat - Risk of solidification issues âœ“\")\n",
    "print(\"\\nAll trigger types align with known steel casting defect mechanisms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Assessment and Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n1. Missing Value Analysis:\")\n",
    "missing_counts = time_series_data.isnull().sum()\n",
    "print(f\"Missing values per sensor: {missing_counts.to_dict()}\")\n",
    "print(f\"Total missing values: {missing_counts.sum()}\")\n",
    "\n",
    "# Data consistency checks\n",
    "print(\"\\n2. Data Consistency Checks:\")\n",
    "for sensor in sensor_columns:\n",
    "    sensor_data = time_series_data[sensor]\n",
    "    within_bounds = (sensor_data >= sensor_data.min()) & (sensor_data <= sensor_data.max())\n",
    "    print(f\"{sensor}: {within_bounds.sum()}/{len(sensor_data)} values within expected range\")\n",
    "\n",
    "# Temporal continuity\n",
    "print(\"\\n3. Temporal Continuity Analysis:\")\n",
    "cast_durations = []\n",
    "for cast_id in metadata['cast_id'][:10]:  # Sample first 10 casts\n",
    "    cast_data = time_series_data[time_series_data['cast_id'] == cast_id]\n",
    "    if not cast_data.empty:\n",
    "        duration = (cast_data.index.max() - cast_data.index.min()).total_seconds() / 60\n",
    "        cast_durations.append(duration)\n",
    "\n",
    "print(f\"Average cast duration: {np.mean(cast_durations):.1f} minutes\")\n",
    "print(f\"Expected duration: 120 minutes\")\n",
    "print(f\"Duration consistency: {'âœ“' if abs(np.mean(cast_durations) - 120) < 1 else 'âœ—'}\")\n",
    "\n",
    "# Outlier detection\n",
    "print(\"\\n4. Outlier Detection (IQR method):\")\n",
    "outlier_summary = []\n",
    "for sensor in sensor_columns:\n",
    "    Q1 = stats_df[f'{sensor}_mean'].quantile(0.25)\n",
    "    Q3 = stats_df[f'{sensor}_mean'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = stats_df[(stats_df[f'{sensor}_mean'] < lower_bound) | \n",
    "                       (stats_df[f'{sensor}_mean'] > upper_bound)]\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'sensor': sensor,\n",
    "        'outlier_count': len(outliers),\n",
    "        'outlier_percentage': len(outliers) / len(stats_df) * 100,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "display(outlier_df)\n",
    "\n",
    "# Synthetic data realism assessment\n",
    "print(\"\\n5. Synthetic Data Realism Assessment:\")\n",
    "print(\"Evaluating against expected steel casting behavior:\")\n",
    "print(f\"â€¢ Casting speed range: {time_series_data['casting_speed'].min():.2f} - {time_series_data['casting_speed'].max():.2f} m/min\")\n",
    "print(f\"  Expected: 0.8 - 1.8 m/min âœ“\")\n",
    "print(f\"â€¢ Mold temperature range: {time_series_data['mold_temperature'].min():.0f} - {time_series_data['mold_temperature'].max():.0f} Â°C\")\n",
    "print(f\"  Expected: 1480 - 1580 Â°C âœ“\")\n",
    "print(f\"â€¢ Superheat range: {time_series_data['superheat'].min():.1f} - {time_series_data['superheat'].max():.1f} Â°C\")\n",
    "print(f\"  Expected: 15 - 40 Â°C âœ“\")\n",
    "print(\"\\nAll sensor ranges are within realistic steel casting operational bounds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Engineering Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FEATURE ENGINEERING RECOMMENDATIONS ===\")\n",
    "\n",
    "# Statistical features analysis\n",
    "print(\"\\n1. Most Informative Statistical Features:\")\n",
    "feature_importance = []\n",
    "\n",
    "for sensor in sensor_columns:\n",
    "    # Calculate separation between good and defect classes\n",
    "    good_vals = stats_df[stats_df['defect_label'] == 0][f'{sensor}_mean']\n",
    "    defect_vals = stats_df[stats_df['defect_label'] == 1][f'{sensor}_mean']\n",
    "    \n",
    "    # Calculate effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt(((len(good_vals)-1)*good_vals.var() + (len(defect_vals)-1)*defect_vals.var()) / \n",
    "                        (len(good_vals) + len(defect_vals) - 2))\n",
    "    cohens_d = abs(good_vals.mean() - defect_vals.mean()) / pooled_std\n",
    "    \n",
    "    feature_importance.append({\n",
    "        'sensor': sensor,\n",
    "        'cohens_d_mean': cohens_d,\n",
    "        'mean_separation': abs(good_vals.mean() - defect_vals.mean()),\n",
    "        'p_value': stats.ttest_ind(good_vals, defect_vals)[1]\n",
    "    })\n",
    "\n",
    "# Add std deviation analysis\n",
    "for sensor in sensor_columns:\n",
    "    good_std = stats_df[stats_df['defect_label'] == 0][f'{sensor}_std']\n",
    "    defect_std = stats_df[stats_df['defect_label'] == 1][f'{sensor}_std']\n",
    "    \n",
    "    pooled_std = np.sqrt(((len(good_std)-1)*good_std.var() + (len(defect_std)-1)*defect_std.var()) / \n",
    "                        (len(good_std) + len(defect_std) - 2))\n",
    "    cohens_d_std = abs(good_std.mean() - defect_std.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    # Find corresponding sensor in feature_importance\n",
    "    for item in feature_importance:\n",
    "        if item['sensor'] == sensor:\n",
    "            item['cohens_d_std'] = cohens_d_std\n",
    "            break\n",
    "\n",
    "feature_df = pd.DataFrame(feature_importance)\n",
    "feature_df = feature_df.sort_values('cohens_d_mean', ascending=False)\n",
    "\n",
    "print(\"Sensor ranking by discriminative power (Cohen's d):\")\n",
    "display(feature_df)\n",
    "\n",
    "print(\"\\n2. Recommended Feature Categories:\")\n",
    "print(\"\\nA. Time Domain Features:\")\n",
    "print(\"   â€¢ Mean, median, standard deviation, min, max\")\n",
    "print(\"   â€¢ Range (max - min), interquartile range\")\n",
    "print(\"   â€¢ Skewness and kurtosis for distribution shape\")\n",
    "\n",
    "print(\"\\nB. Temporal Stability Features:\")\n",
    "print(\"   â€¢ Number of threshold excursions\")\n",
    "print(\"   â€¢ Rate of change (first derivative)\")\n",
    "print(\"   â€¢ Time above/below operational limits\")\n",
    "\n",
    "print(\"\\nC. Cross-Sensor Features:\")\n",
    "print(\"   â€¢ Temperature-speed ratios\")\n",
    "print(\"   â€¢ Mold level deviation duration\")\n",
    "print(\"   â€¢ Cooling efficiency indicators\")\n",
    "\n",
    "print(\"\\nD. Domain-Specific Features:\")\n",
    "print(\"   â€¢ Superheat adequacy index\")\n",
    "print(\"   â€¢ Thermal gradient indicators\")\n",
    "print(\"   â€¢ Process stability scores\")\n",
    "\n",
    "print(\"\\n3. Time Window Recommendations:\")\n",
    "print(\"   â€¢ For LSTM: 60-120 second sequences (1-2 minutes)\")\n",
    "print(\"   â€¢ For baseline models: Full cast aggregation (120 minutes)\")\n",
    "print(\"   â€¢ For real-time: Rolling 30-second windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Development Insights and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL DEVELOPMENT INSIGHTS ===\")\n",
    "\n",
    "print(\"\\n1. BASELINE MODEL RECOMMENDATIONS:\")\n",
    "print(\"   â€¢ Focus on statistical aggregation features\")\n",
    "print(f\"   â€¢ Top discriminative sensors: {', '.join(feature_df.head(3)['sensor'].tolist())}\")\n",
    "print(\"   â€¢ Include interaction terms between sensors\")\n",
    "print(\"   â€¢ Apply feature scaling due to different sensor ranges\")\n",
    "\n",
    "print(\"\\n2. LSTM MODEL RECOMMENDATIONS:\")\n",
    "print(\"   â€¢ Sequence length: 60-120 time steps (1-2 minutes)\")\n",
    "print(\"   â€¢ Multi-sensor input with 5 features per time step\")\n",
    "print(\"   â€¢ Bidirectional LSTM to capture forward/backward dependencies\")\n",
    "print(\"   â€¢ Attention mechanism for critical time period identification\")\n",
    "\n",
    "print(\"\\n3. CLASS IMBALANCE HANDLING:\")\n",
    "current_ratio = metadata['defect_label'].mean()\n",
    "print(f\"   â€¢ Current defect rate: {current_ratio:.1%}\")\n",
    "print(\"   â€¢ Recommended techniques:\")\n",
    "print(\"     - SMOTE for synthetic minority class generation\")\n",
    "print(\"     - Class weights in loss function\")\n",
    "print(\"     - Stratified sampling for train/validation splits\")\n",
    "\n",
    "print(\"\\n4. VALIDATION STRATEGY:\")\n",
    "print(\"   â€¢ Time-based split to avoid data leakage\")\n",
    "print(\"   â€¢ Stratified sampling to maintain defect rate\")\n",
    "print(\"   â€¢ Cross-validation with temporal awareness\")\n",
    "\n",
    "print(\"\\n5. KEY FINDINGS SUMMARY:\")\n",
    "print(f\"   â€¢ Generated {dataset_info['dataset_info']['total_casts']} casts with {current_ratio:.1%} defect rate\")\n",
    "print(f\"   â€¢ {len(trigger_counts)} distinct trigger mechanisms identified\")\n",
    "print(f\"   â€¢ Strong correlation patterns differ between good/defect casts\")\n",
    "print(f\"   â€¢ All sensors show realistic operational ranges\")\n",
    "print(f\"   â€¢ No missing values or temporal discontinuities detected\")\n",
    "\n",
    "print(\"\\n6. NEXT STEPS FOR PHASE 2:\")\n",
    "print(\"   âœ“ Implement feature engineering pipeline\")\n",
    "print(\"   âœ“ Develop baseline XGBoost model\")\n",
    "print(\"   âœ“ Create LSTM architecture\")\n",
    "print(\"   âœ“ Establish model evaluation framework\")\n",
    "print(\"   âœ“ Design real-time inference system\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA EXPLORATION FRAMEWORK COMPLETED SUCCESSFULLY\")\n",
    "print(\"Ready for Phase 2: Feature Engineering and Model Development\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}