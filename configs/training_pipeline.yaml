training_pipeline:
  # Experiment Configuration
  experiment:
    name: "baseline_steel_defect_prediction"
    description: "Baseline model training for steel casting defect prediction"
    tags: ["baseline", "xgboost", "phase2"]
    
  # Data Configuration
  data:
    target_column: "defect"
    feature_columns: null  # null means use all except target
    test_size: 0.2
    validation_size: 0.2
    stratify: true
    random_state: 42
    
  # Preprocessing Configuration
  preprocessing:
    handle_missing: true
    missing_strategy: "median"  # mean, median, mode, drop
    categorical_missing_strategy: "most_frequent"
    
    # Feature scaling
    scaling:
      method: "standard"  # standard, robust, minmax, none
      feature_range: [0, 1]  # for minmax scaling
      
    # Outlier handling
    outliers:
      method: "iqr"  # iqr, zscore, isolation_forest, none
      threshold: 1.5
      
    # Feature selection
    feature_selection:
      enabled: false
      method: "variance"  # variance, univariate, recursive
      k_best: 50
      
  # Model Configuration
  model:
    type: "xgboost"
    parameters:
      objective: "binary:logistic"
      eval_metric: "auc"
      n_estimators: 100
      max_depth: 6
      learning_rate: 0.1
      subsample: 0.8
      colsample_bytree: 0.8
      random_state: 42
      n_jobs: -1
      
  # Training Configuration
  training:
    # Cross-validation
    cross_validation:
      enabled: true
      cv_folds: 5
      stratify: true
      shuffle: true
      
    # Early stopping
    early_stopping:
      enabled: true
      monitor: "val_auc"
      patience: 10
      min_delta: 0.001
      restore_best_weights: true
      
    # Class balancing
    class_balancing:
      method: "weights"  # weights, smote, undersampling, none
      ratio: "balanced"
      
  # Hyperparameter Search
  hyperparameter_search:
    enabled: true
    method: "grid"  # grid, random, bayesian
    cv_folds: 3
    scoring: "roc_auc"
    n_jobs: -1
    
    # Parameter grids
    param_grids:
      coarse:
        n_estimators: [50, 100, 200]
        max_depth: [3, 6, 9]
        learning_rate: [0.05, 0.1, 0.2]
        
      fine:
        n_estimators: [80, 100, 120]
        max_depth: [5, 6, 7]
        learning_rate: [0.08, 0.1, 0.12]
        subsample: [0.7, 0.8, 0.9]
        colsample_bytree: [0.7, 0.8, 0.9]
        
    # Random search parameters
    random_search:
      n_iter: 100
      
    # Bayesian optimization parameters
    bayesian_search:
      n_calls: 50
      acq_func: "EI"  # EI, LCB, PI
      
  # Evaluation Configuration
  evaluation:
    metrics: ["roc_auc", "average_precision", "f1", "precision", "recall"]
    threshold_optimization: true
    plot_curves: true
    confusion_matrix: true
    
  # Output Configuration
  output:
    model_dir: "models/artifacts"
    results_dir: "results/training"
    plots_dir: "plots/training"
    logs_dir: "logs"
    
    # Model versioning
    versioning:
      enabled: true
      format: "{model_type}_{timestamp}_{performance}"
      
    # Artifacts to save
    save_artifacts:
      model: true
      preprocessor: true
      training_history: true
      feature_importance: true
      evaluation_plots: true
      
  # Logging Configuration
  logging:
    level: "INFO"
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    file: "logs/training_pipeline.log"
    console: true