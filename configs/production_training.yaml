# Production Training Configuration
# Optimized settings for final production models

# Data Configuration
data:
  target_column: "defect"
  test_size: 0.15      # Smaller test set to maximize training data
  validation_size: 0.15
  random_state: 42
  handle_missing: true

# Model Configuration
model:
  type: "xgboost"
  parameters:
    objective: "binary:logistic"
    eval_metric: "auc"
    n_estimators: 200    # More trees for better performance
    max_depth: 8         # Deeper trees
    learning_rate: 0.05  # Lower learning rate for stability
    subsample: 0.8
    colsample_bytree: 0.8
    min_child_weight: 1
    gamma: 0.1           # Some regularization
    reg_alpha: 0.1
    reg_lambda: 1.0
    random_state: 42
    n_jobs: -1
    verbosity: 0

# Training Configuration
training:
  hyperparameter_search: true
  search_method: "bayesian"  # More sophisticated search
  early_stopping_patience: 20
  cross_validation: true
  cv_folds: 10              # More thorough validation
  early_stopping: true
  early_stopping_rounds: 20

# Preprocessing Configuration
preprocessing:
  scaling_method: "standard"
  handle_missing: true
  missing_strategy: "median"
  categorical_strategy: "most_frequent"

# Hyperparameter Search Configuration
hyperparameter_search:
  enabled: true
  method: "bayesian"
  n_iter: 100             # Extensive search
  cv_folds: 10
  scoring: "roc_auc"
  param_grids:
    production:
      n_estimators: [150, 200, 300, 500]
      max_depth: [6, 8, 10, 12]
      learning_rate: [0.01, 0.05, 0.1]
      subsample: [0.7, 0.8, 0.9]
      colsample_bytree: [0.7, 0.8, 0.9]
      min_child_weight: [1, 3, 5]
      gamma: [0, 0.1, 0.2, 0.5]
      reg_alpha: [0, 0.1, 0.5, 1.0]
      reg_lambda: [0.5, 1.0, 2.0]

# Output Configuration
output:
  save_artifacts: true
  experiment_name_prefix: "production"
  output_dir: "models/production"

# Execution Configuration
execution:
  verbose: false         # Less verbose for production
  n_jobs: -1
  random_state: 42
